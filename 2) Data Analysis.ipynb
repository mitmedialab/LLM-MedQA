{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"913923b3f06c4aecaa10d62feb283d8e","deepnote_cell_type":"text-cell-h1"},"source":"# 2) Data Analysis"},{"cell_type":"code","metadata":{"source_hash":"45db2d19","execution_start":1686942524768,"execution_millis":5,"deepnote_to_be_reexecuted":false,"cell_id":"e748e05b9e074de2946a56ab0f895494","deepnote_cell_type":"code"},"source":"import pandas as pd\nimport random\nimport os\nimport openai\nimport json\nimport re\nimport matplotlib.pyplot as plt","execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"df4861fe","execution_start":1686942528338,"execution_millis":144,"deepnote_to_be_reexecuted":false,"cell_id":"fd7dfbe67ee64b23855387cb335c6761","deepnote_cell_type":"code"},"source":"#Load Dataset\nExperiment_1 = pd.read_csv(\"/work/2) Cleaned Experiment Data/Cleaned - Experiment 1.csv\")\nExperiment_2 = pd.read_csv(\"/work/2) Cleaned Experiment Data/Cleaned - Experiment 2.csv\")\nExperiment_3 = pd.read_csv(\"/work/2) Cleaned Experiment Data/Cleaned - Experiment 3.csv\")\n\nResponse_Source = [\"_D_\", \"_H_\", \"_L_\"] # _D_ = Doctor, _H_ = High Accuracy AI-Generated, _L_ = Low Accuracy AI-Generated\nMedical_Domain = [\"_Preventative_\", \"_Conditions_\", \"_Diagnostic\", \"_Procedures_\", \"_Medications_\", \"_Recovery_\"] # 6 Different medical domains\n\nQuestion_Names_Exp_1 = [\"Und_Q\", \"Und_R\", \"AI_or_Human\", \"Confidence\"] # Question Names in Experiment 1\nQuestion_Type_Exp_1 = [\"_2_1\", \"_4_1\", \"_6\", \"_7_1\"] # Number labels for each question in the raw dataset\n\nQuestion_Names_Exp_2_3 = [\"Und_Q\", \"Und_R\", \"Valid\", \"Trust\", \"Satis\", \"Follow\", \"Action\", \"Info\"]# Question Names in Experiment 2\nQuestion_Type_Exp_2_3 = [\"_2_1\", \"_4_1\", \"_6\", \"_7_1\", \"_8_1\", \"_10_1\", \"_11_1\", \"_12_1\"] # Number labels for each question in the raw dataset\n","execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"underline":true},"toCodePoint":12,"fromCodePoint":0}],"cell_id":"d564959321bc4dcb837ebb0c7440a574","deepnote_cell_type":"text-cell-h3"},"source":"### Experiment 1 - Data Analysis"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":15,"fromCodePoint":0}],"cell_id":"2521615da7614c26a7aee20855b573f4","deepnote_cell_type":"text-cell-p"},"source":"Experiment 1 - Dataset Extraction"},{"cell_type":"code","metadata":{"source_hash":"e7f983a3","execution_start":1686942553217,"execution_millis":6,"deepnote_to_be_reexecuted":false,"cell_id":"17ecfdd5c22e447a90ef66c6dbb54558","deepnote_cell_type":"code"},"source":"Exp1_Lists = {\"Und_Q\": [], \"Und_R\": [], \"AI_or_Human\": [], \"Confidence\": [], \"_D_\": [], \"_H_\": [], \"_L_\": [], \"_Preventative_\": [], \"_Conditions_\": [], \"_Diagnostic\": [], \"_Procedures_\": [], \"_Medications_\": [], \"_Recovery_\":[]} \n\n# Experiment 1 - Dictionary of empty lists for each question type to organize the different columns from the raw dataset into their respective question type\n\nExperiment_1_df = pd.DataFrame(Experiment_1)\ncolumn_headers = list(Experiment_1_df)\n\nfor column in column_headers:\n    for i in range(len(Question_Type_Exp_1)): # Organizing raw dataset columns into participant evaluation question type\n        question = Question_Type_Exp_1[i]\n        ques_title = Question_Names_Exp_1[i]\n        if question in column:\n            Exp1_Lists[ques_title].append(column)       \n        \n    for source in Response_Source: # Organizing raw dataset columns into medical response source\n        if source in column:\n            Exp1_Lists[source].append(column)\n    \n    for domain in Medical_Domain: # Organizing raw dataset columns into medical response domain\n        if domain in column:\n            Exp1_Lists[domain].append(column)\n\n\n\n# Creating empty dataset for Experiment 1 to fill and organize data from raw dataset\n\n# (Understanding Question & Response results)\ndataset_exp_1 = pd.DataFrame()\ndataset_exp_1.insert(0, \"Question Type\", 0)\ndataset_exp_1.insert(1, \"Response Scores\", 0)\ndataset_exp_1.insert(2, \"Response Source\", 0)\ndataset_exp_1.insert(3, \"Participant ID\", 0)\ndataset_exp_1.insert(4, \"Question ID\", 0)\n\n# (Determining response source - AI or Human - and Confidence  results)\ndataset_exp_1_2 = pd.DataFrame()\ndataset_exp_1_2.insert(0, \"Question Type\", 0)\ndataset_exp_1_2.insert(1, \"Incorrect/Correct\", 0)\ndataset_exp_1_2.insert(2, \"Response Scores\", 0)\ndataset_exp_1_2.insert(3, \"Confidence\", 0)\ndataset_exp_1_2.insert(4, \"Response Source\", 0)\ndataset_exp_1_2.insert(5, \"Participant ID\", 0)\ndataset_exp_1_2.insert(6, \"Question ID\", 0)\n","execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":15,"fromCodePoint":0}],"cell_id":"0cb5f9b9b231455d986579fda83ebea0","deepnote_cell_type":"text-cell-p"},"source":"Experiment 1 - Functions"},{"cell_type":"code","metadata":{"source_hash":"f71e382","execution_start":1686942556171,"execution_millis":2,"deepnote_to_be_reexecuted":false,"cell_id":"b33db908064c464d8235bd46bfbe70e7","deepnote_cell_type":"code"},"source":"# Function to identify if participant answer of AI or Human matches the true source of the response shown to them\n\ndef AI_or_Human(Question, Dataset, Stat_dataset, Survey_Dict) :\n\n    Source_Str = [\"Doctor\", \"High Accuracy AI\", \"Low Accuracy AI\"]\n\n    for i in range(len(Response_Source)):\n        Source = Response_Source[i]\n        Source_list = Survey_Dict[Source]\n        Ques_list = Survey_Dict[Question]\n\n        Sample = sorted(list(set(Ques_list) & set(Source_list)))\n\n        for col in Sample:\n            Question_ID = col\n            Temp_column = Dataset.loc[:, col]\n            conf_str = str(col)\n            conf = conf_str[0:-1] + '7_1'\n            Conf_column = Dataset.loc[:, conf]\n            Participants = Dataset.loc[:, \"prolific_id\"]\n\n            for j in range(len(Temp_column)):\n                resp = Temp_column[j]\n                confidence = Conf_column[j]\n                Participant_ID = Participants[j]\n\n                try: \n                    if str(resp) == \"Doctor\":\n                        if Source_Str[i] == \"Doctor\":\n                            Stat_dataset.loc[len(Stat_dataset)] = [Question, \"Correct\", 1, confidence, Source_Str[i], Participant_ID, Question_ID]\n                        if Source_Str[i] == \"High Accuracy AI\":\n                            Stat_dataset.loc[len(Stat_dataset)] = [Question, \"Incorrect\", 0, confidence, Source_Str[i], Participant_ID, Question_ID]\n                        if Source_Str[i] == \"Low Accuracy AI\":\n                            Stat_dataset.loc[len(Stat_dataset)] = [Question, \"Incorrect\", 0, confidence, Source_Str[i], Participant_ID, Question_ID]\n\n                    if str(resp) == \"AI Text Generator\":\n                        if Source_Str[i] == \"Doctor\":\n                            Stat_dataset.loc[len(Stat_dataset)] = [Question, \"Incorrect\", 0, confidence, Source_Str[i], Participant_ID, Question_ID]\n                        if Source_Str[i] == \"High Accuracy AI\":\n                            Stat_dataset.loc[len(Stat_dataset)] = [Question, \"Correct\", 1, confidence, Source_Str[i], Participant_ID, Question_ID]\n                        if Source_Str[i] == \"Low Accuracy AI\":\n                            Stat_dataset.loc[len(Stat_dataset)] = [Question, \"Correct\", 1, confidence, Source_Str[i], Participant_ID, Question_ID]\n\n                except:\n                    pass\n\n    return('Done')\n\n\n\n\n# Function for analyzing likert scale participant responses\n\ndef Likert(Question, Dataset, Stat_dataset, Survey_Str, Survey_Dict) :\n\n    Source_Str = [\"Doctor\", \"High Accuracy AI\", \"Low Accuracy AI\"]\n\n    for i in range(len(Response_Source)):\n        Source = Response_Source[i]\n        Source_list = Survey_Dict[Source]\n        Ques_list = Survey_Dict[Question]\n\n        Sample = sorted(list(set(Ques_list) & set(Source_list)))\n     \n        for col in Sample:\n            Question_ID = col\n            Temp_column = Dataset.loc[:, col]\n            Participants = Dataset.loc[:, \"prolific_id\"]\n        \n            for j in range(len(Temp_column)):\n                resp = Temp_column[j]\n                Participant_ID = Participants[j]\n                try:                \n                    if 0 <= float(resp) <= 5:\n                        Stat_dataset.loc[len(Stat_dataset)] = [Question, float(resp), Source_Str[i], Participant_ID, Question_ID]\n                except:\n                    pass\n\n    return('Done')\n","execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":15,"fromCodePoint":0}],"cell_id":"619fca5bafe44402a49098ae16a00b80","deepnote_cell_type":"text-cell-p"},"source":"Experiment 1 - Execution"},{"cell_type":"code","metadata":{"source_hash":"86518dcc","execution_start":1686942562645,"execution_millis":8837,"deepnote_to_be_reexecuted":false,"cell_id":"06c7289cdce64006b73e05345d9d51fb","deepnote_cell_type":"code"},"source":"Likert(\"Und_Q\", Experiment_1, dataset_exp_1, \"Experiment 1\", Exp1_Lists)\nLikert(\"Und_R\", Experiment_1, dataset_exp_1, \"Experiment 1\", Exp1_Lists)\nAI_or_Human(\"AI_or_Human\", Experiment_1, dataset_exp_1_2, Exp1_Lists)","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"'Done'"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"underline":true},"toCodePoint":12,"fromCodePoint":0}],"cell_id":"2aad6d9110df468f9501a9152a08696c","deepnote_cell_type":"text-cell-h3"},"source":"### Experiment 2 - Data Analysis"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":15,"fromCodePoint":0}],"cell_id":"47f27f93dd7449148c513fb9f23d225c","deepnote_cell_type":"text-cell-p"},"source":"Experiment 2 - Dataset Extraction"},{"cell_type":"code","metadata":{"source_hash":"3a870027","execution_start":1686941591836,"execution_millis":4,"deepnote_to_be_reexecuted":false,"cell_id":"a26af218059241e9ab3a670c01d2a47c","deepnote_cell_type":"code"},"source":"Exp2_Lists = {\"Und_Q\": [], \"Und_R\": [], \"Valid\": [], \"Trust\": [], \"Satis\": [], \"Follow\": [], \"Action\": [], \"Info\": [], \"_D_\": [], \"_H_\": [], \"_L_\": [], \"_Preventative_\": [], \"_Conditions_\": [], \"_Diagnostic\": [], \"_Procedures_\": [], \"_Medications_\": [], \"_Recovery_\":[]}\n\n# Experiment 2 - Dictionary of empty lists for each question type to organize the different columns from the raw dataset into their respective question type\n\nExperiment_2_df = pd.DataFrame(Experiment_2)\ncolumn_headers = list(Experiment_2_df)\n\nfor column in column_headers:\n    for i in range(len(Question_Type_Exp_2_3)): # Organizing raw dataset columns into participant evaluation question type\n        question = Question_Type_Exp_2_3[i]\n        ques_title = Question_Names_Exp_2_3[i]\n        if question in column:\n            Exp2_Lists[ques_title].append(column)       \n        \n    for source in Response_Source: # Organizing raw dataset columns into medical response source\n        if source in column:\n            Exp2_Lists[source].append(column)\n    \n    for domain in Medical_Domain: # Organizing raw dataset columns into medical response domain\n        if domain in column:\n            Exp2_Lists[domain].append(column)\n\n\n# Creating empty dataset for Experiment 2 to fill and organize data from raw dataset\ndataset_exp_2 = pd.DataFrame()\ndataset_exp_2.insert(0, \"Question Type\", 0)\ndataset_exp_2.insert(1, \"Response Scores\", 0)\ndataset_exp_2.insert(2, \"Response Source\", 0)\ndataset_exp_2.insert(3, \"Participant ID\", 0)\ndataset_exp_2.insert(4, \"Question ID\", 0)","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":15,"fromCodePoint":0}],"cell_id":"2c584e45a870471e8da9589c4864750b","deepnote_cell_type":"text-cell-p"},"source":"Experiment 2 - Functions"},{"cell_type":"code","metadata":{"source_hash":"c2ea4ae7","execution_start":1686941591837,"execution_millis":4,"deepnote_to_be_reexecuted":false,"cell_id":"b03d2f05faf647308db53e1f45d4da3f","deepnote_cell_type":"code"},"source":"# Function for analyzing yes / no participant responses (i.e. Validity Responses)\n\ndef yes_or_no(Question, Dataset, Stat_dataset, Survey_Dict) :\n\n    Source_Str = [\"Doctor\", \"High Accuracy AI\", \"Low Accuracy AI\"]\n\n    for i in range(len(Response_Source)):\n        Source = Response_Source[i]\n        Source_list = Survey_Dict[Source]\n        Ques_list = Survey_Dict[Question]\n\n        Sample = sorted(list(set(Ques_list) & set(Source_list)))\n\n        for col in Sample:\n            Question_ID = col\n            Temp_column = Dataset.loc[:, col]\n            Participants = Dataset.loc[:, \"prolific_id\"]\n        \n            for j in range(len(Temp_column)):\n                Participant_ID = Participants[j]\n                resp = Temp_column[j]\n                try:                \n                    if str(resp) == \"Yes\":\n                        Stat_dataset.loc[len(Stat_dataset)] = [Question, 1, Source_Str[i], Participant_ID, Question_ID]\n                    if str(resp) == \"No\":\n                        Stat_dataset.loc[len(Stat_dataset)] = [Question, 0, Source_Str[i], Participant_ID, Question_ID]\n\n                except:\n                    pass\n\n    return('Done')\n\n# Use likert function from earlier to analyze likert scale participant responses","execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":15,"fromCodePoint":0}],"cell_id":"58834e40df2549a3bd79b33037024c34","deepnote_cell_type":"text-cell-p"},"source":"Experiment 2 - Execution"},{"cell_type":"code","metadata":{"source_hash":"88e7c78f","execution_start":1686941591858,"execution_millis":22313,"deepnote_to_be_reexecuted":false,"cell_id":"5860e38aabed4936a17d876443bab7a4","deepnote_cell_type":"code"},"source":"for ques in Question_Names_Exp_2_3:\n    if ques != \"Valid\":\n        Likert(ques, Experiment_2, dataset_exp_2, \"Experiment 2\", Exp2_Lists)\n    else:\n        yes_or_no(ques, Experiment_2, dataset_exp_2, Exp2_Lists)","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"underline":true},"toCodePoint":12,"fromCodePoint":0}],"cell_id":"a43eec45e9474a1ab7acbfb00b6a3cb5","deepnote_cell_type":"text-cell-h3"},"source":"### Experiment 3 - Data Analysis"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":15,"fromCodePoint":0}],"cell_id":"53fff208797544ac91680bacd459f6ae","deepnote_cell_type":"text-cell-p"},"source":"Experiment 3 - Dataset Extraction"},{"cell_type":"code","metadata":{"source_hash":"927999b6","execution_start":1686941622188,"execution_millis":8,"deepnote_to_be_reexecuted":false,"cell_id":"9f10f92c06784f3282ae90669874a9e7","deepnote_cell_type":"code"},"source":"Random_Source = Experiment_3.loc[:, \"Assigned_Conditions\"]\n\n# Experiment 3 - Dictionary of empty lists for each question type to organize the different columns from the raw dataset into their respective question type\n\nExp3_Lists = {\"Und_Q\": [], \"Und_R\": [], \"Valid\": [], \"Trust\": [], \"Satis\": [], \"Follow\": [], \"Action\": [], \"Info\": [], \"_D_\": [], \"_H_\": [], \"_L_\": [], \"_Preventative_\": [], \"_Conditions_\": [], \"_Diagnostic\": [], \"_Procedures_\": [], \"_Medications_\": [], \"_Recovery_\":[]}\n\nExperiment_3_df = pd.DataFrame(Experiment_3)\ncolumn_headers = list(Experiment_3_df)\n\nfor column in column_headers: # Organizing raw dataset columns into participant evaluation question type\n    for i in range(len(Question_Type_Exp_2_3)):\n        question = Question_Type_Exp_2_3[i]\n        ques_title = Question_Names_Exp_2_3[i]\n        if question in column:\n            Exp3_Lists[ques_title].append(column)       \n        \n    for source in Response_Source: # Organizing raw dataset columns into medical response source\n        if source in column:\n            Exp3_Lists[source].append(column)\n    \n    for domain in Medical_Domain: # Organizing raw dataset columns into medical response domain\n        if domain in column:\n            Exp3_Lists[domain].append(column)\n\n\n# Creating empty dataset for Experiment 1 to fill and organize data from raw dataset\ndataset_exp_3 = pd.DataFrame()\ndataset_exp_3.insert(0, \"Question Type\", 0)\ndataset_exp_3.insert(1, \"Response Scores\", 0)\ndataset_exp_3.insert(2, \"Response Source\", 0)\ndataset_exp_3.insert(3, \"Random Header\", 0)\ndataset_exp_3.insert(4, \"Participant ID\", 0)\ndataset_exp_3.insert(5, \"Question ID\", 0)\n","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":15,"fromCodePoint":0}],"cell_id":"20876f3ea82443c0a78290ea338c8ee1","deepnote_cell_type":"text-cell-p"},"source":"Experiment 3 - Functions"},{"cell_type":"code","metadata":{"source_hash":"de029817","execution_start":1686941622237,"execution_millis":2,"deepnote_to_be_reexecuted":false,"cell_id":"63af44d894b44708bf1a85d92f72f026","deepnote_cell_type":"code"},"source":"# Function for analyzing likert scale participant responses in Experiment 3 --> made specific to take into account the three different random lables presented (\"Doctor\", \"AI\", \"Doctor-assisted by AI\")\n\ndef Indicated_Source_Likert(Question, Dataset, Stat_dataset, Survey_Str, Survey_Dict) :\n    \n    Add = [\" - D\", \" - H\", \" - L\"]\n    Ques = Survey_Dict[Question]\n\n    for i in range(3):\n        name = Question + Add[i]\n        curr_set = Survey_Dict[Response_Source[i]]\n        Sample = sorted(list(set(Ques) & set(curr_set)))\n\n        for col in Sample:\n            Temp_column = Dataset.loc[:, col]\n            Question_ID = col\n            Participants = Dataset.loc[:, \"prolific_id\"]\n\n            for ind in Dataset.index:\n                resp = Temp_column[ind]\n                Participant_ID = Participants[ind]\n                if str(Random_Source[ind]) == \"DOCTOR\":\n                    if 0 <= float(resp) <= 5:\n                        Stat_dataset.loc[len(Stat_dataset)] = [Question, float(resp), name, \"Doctor\", Participant_ID, Question_ID]\n                    \n                if str(Random_Source[ind]) == \"ARTIFICIAL INTELLIGENCE (A.I.)\":\n                    if 0 <= float(resp) <= 5:\n                        Stat_dataset.loc[len(Stat_dataset)] = [Question, float(resp), name, \"AI\", Participant_ID, Question_ID]\n                \n                if str(Random_Source[ind]) == \"DOCTOR ASSISTED BY ARTIFICIAL INTELLIGENCE (A.I.)\":\n                    if 0 <= float(resp) <= 5:\n                        Stat_dataset.loc[len(Stat_dataset)] = [Question, float(resp), name, \"Doctor + AI\", Participant_ID, Question_ID]\n\n    return(\"Done\")\n\n\n\n# Function for analyzing yes / no participant responses (i.e. Validity responses) in Experiment 3 --> made specific to take into account the three different random lables presented (\"Doctor\", \"AI\", \"Doctor-assisted by AI\")\n\ndef Yes_or_No_Exp3(Question, Dataset, Stat_dataset, Survey_Dict):\n    \n    Add = [\" - D\", \" - H\", \" - L\"]\n    Ques = Survey_Dict[Question]\n\n    for i in range(3):\n        name = Question + Add[i]\n        curr_set = Survey_Dict[Response_Source[i]]\n        Sample = sorted(list(set(Ques) & set(curr_set)))\n\n        for col in Sample:\n            Temp_column = Dataset.loc[:, col]\n            Question_ID = col\n            Participants = Dataset.loc[:, \"prolific_id\"]\n\n            for ind in Dataset.index:\n                resp = Temp_column[ind]\n                Participant_ID = Participants[ind]\n\n                if str(Random_Source[ind]) == \"DOCTOR\":\n                    if resp == \"Yes\":\n                        Stat_dataset.loc[len(Stat_dataset)] = [Question, 1, name, \"Doctor\", Participant_ID, Question_ID]\n\n                    if resp == \"No\":\n                        Stat_dataset.loc[len(Stat_dataset)] = [Question, 0, name, \"Doctor\", Participant_ID, Question_ID]\n                    \n\n                if str(Random_Source[ind]) == \"ARTIFICIAL INTELLIGENCE (A.I.)\":\n                    if resp == \"Yes\":\n                        Stat_dataset.loc[len(Stat_dataset)] = [Question, 1, name, \"AI\", Participant_ID, Question_ID]\n\n                    if resp == \"No\":\n                        Stat_dataset.loc[len(Stat_dataset)] = [Question, 0, name, \"AI\", Participant_ID, Question_ID]\n                    \n\n                if str(Random_Source[ind]) == \"DOCTOR ASSISTED BY ARTIFICIAL INTELLIGENCE (A.I.)\":\n                    if resp == \"Yes\":\n                        Stat_dataset.loc[len(Stat_dataset)] = [Question, 1, name, \"Doctor + AI\", Participant_ID, Question_ID]\n\n                    if resp == \"No\":\n                        Stat_dataset.loc[len(Stat_dataset)] = [Question, 0, name, \"Doctor + AI\", Participant_ID, Question_ID]\n\n    return(\"Done\")\n","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":15,"fromCodePoint":0}],"cell_id":"7710bd845acd4d8dbc823da64964b42e","deepnote_cell_type":"text-cell-p"},"source":"Experiment 3 - Execution"},{"cell_type":"code","metadata":{"source_hash":"c61e001d","execution_start":1686941622238,"execution_millis":24634,"deepnote_to_be_reexecuted":false,"cell_id":"aae25b709f6b42dd9bdd26fbd0286a7e","deepnote_cell_type":"code"},"source":"for ques in Question_Names_Exp_2_3:\n    if ques != \"Valid\":\n        Indicated_Source_Likert(ques, Experiment_3, dataset_exp_3, \"Experiment 3\", Exp3_Lists)\n    else:\n        Yes_or_No_Exp3(ques, Experiment_3, dataset_exp_3, Exp3_Lists)\n","execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"23078d74d5124e798a5d91cc1089a031","deepnote_cell_type":"text-cell-h3"},"source":"### Dataset Conversion to CSV Files"},{"cell_type":"code","metadata":{"source_hash":"2ba07bf7","execution_start":1686941681704,"execution_millis":82,"deepnote_to_be_reexecuted":false,"cell_id":"bc38f1dd0e7a48ef91397478a4c4d0bc","deepnote_cell_type":"code"},"source":"dataset_exp_2.to_csv(\"/work/3) Organized Experiment Data/Experiment 2.csv\")\ndataset_exp_1.to_csv(\"/work/3) Organized Experiment Data/Experiment 1.csv\")\ndataset_exp_1_2.to_csv(\"/work/3) Organized Experiment Data/Experiment 1 - 2.csv\")\ndataset_exp_3.to_csv(\"/work/3) Organized Experiment Data/Experiment 3.csv\")","execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"5258023defde4df6bfe2ae27832d6119","deepnote_cell_type":"text-cell-h3"},"source":"### Datasets for Statistical Analysis in R"},{"cell_type":"code","metadata":{"cell_id":"ad2783ffdc044fb889b6243c363470ae","deepnote_cell_type":"code"},"source":"#Load Dataset\nExperiment_1_fd = pd.read_csv(\"/work/data/3) final_data/Survey 2 - 1.csv\")\nExperiment_2_fd = pd.read_csv(\"/work/data/3) final_data/Survey 1.csv\")\nExperiment_1_2_fd = pd.read_csv(\"/work/data/3) final_data/Survey 2 - 2.csv\")\nExperiment_3_fd = pd.read_csv(\"/work/data/3) final_data/Survey 3.csv\")\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"6ecbb191404148189998be0858ae53c1","deepnote_cell_type":"code"},"source":"Survey_1_Und_Q = Survey_1_fd.iloc[range(0, 960), range(2, 6)].copy()\nSurvey_1_Und_R = Survey_1_fd.iloc[range(961, 1920), range(2, 6)].copy()\nSurvey_1_Valid = Survey_1_fd.iloc[range(1921, 2869), range(2, 6)].copy()\nSurvey_1_Trust = Survey_1_fd.iloc[range(2870, 3829), range(2, 6)].copy()\nSurvey_1_Satis = Survey_1_fd.iloc[range(3830, 4789), range(2, 6)].copy()\nSurvey_1_Follow = Survey_1_fd.iloc[range(4790, 5749), range(2, 6)].copy()\nSurvey_1_Action = Survey_1_fd.iloc[range(5750, 6709), range(2, 6)].copy()\nSurvey_1_Info = Survey_1_fd.iloc[range(6710, 7669), range(2, 6)].copy()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c310279d-b439-442d-88c9-6cebee17bd26' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"594a7225fd4943bcaeb1d0db3ee2dd61","deepnote_persisted_session":{"createdAt":"2023-06-16T16:01:58.493Z"},"deepnote_execution_queue":[]}}